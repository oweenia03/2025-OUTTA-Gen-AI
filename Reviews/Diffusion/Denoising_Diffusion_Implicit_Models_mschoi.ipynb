{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIDJCYlVzMJx"
      },
      "outputs": [],
      "source": [
        "# Paper : \"Denoising Diffusion Implicit Models\" by Jonathan Ho et al. (2021)\n",
        "# The code below was written with reference to the paper's official open source code.\n",
        "# Github Repository : https://github.com/ermongroup/ddim/tree/main"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "viIMemc5zSVl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Model"
      ],
      "metadata": {
        "id": "dlbCNwj-zT7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def swish(x):\n",
        "    # Activation function used in DDPM\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "def get_timestep_embedding(t, channel):\n",
        "    \"\"\"\n",
        "    DDPM recieves timestep t as input to estimate the noise value.\n",
        "    This embedding fuction takes timestep t as input and returns embedding vector according to t.\n",
        "\n",
        "    Parameters:\n",
        "        t (torch.Tensor) : Timesteps of batch data\n",
        "        channel (int) : Number of embedding channels\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor : Embedding vector\n",
        "    \"\"\"\n",
        "    half = channel // 2\n",
        "    device = t.device\n",
        "    freqs = torch.exp(\n",
        "        -torch.arange(half, dtype=torch.float32, device=device) * 2.0 * 3.1415 / float(half)\n",
        "    )\n",
        "    embedded = []\n",
        "    for val in t.float():\n",
        "        sin_embed = torch.sin(val * freqs)\n",
        "        cos_embed = torch.cos(val * freqs)\n",
        "        embedded.append(torch.cat([sin_embed, cos_embed], dim=0))\n",
        "    embedded = torch.stack(embedded, dim=0)\n",
        "    if channel % 2 == 1:\n",
        "        embedded = F.pad(embedded, (0,1,0,0))\n",
        "    return embedded  # (B, channel)\n",
        "\n",
        "class GroupNorm32(nn.GroupNorm):\n",
        "    def __init__(self, num_channels, num_groups=32, eps=1e-6):\n",
        "        super().__init__(num_groups, num_channels, eps=eps)\n",
        "\n",
        "def conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True, init_scale=1.0):\n",
        "    conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding, bias=bias)\n",
        "    with torch.no_grad():\n",
        "        conv.weight.data *= init_scale\n",
        "    return conv\n",
        "\n",
        "def nin(in_ch, out_ch, init_scale=1.0):\n",
        "    # 1x1 convolution\n",
        "    layer = nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1, padding=0)\n",
        "    with torch.no_grad():\n",
        "        layer.weight.data *= init_scale\n",
        "    return layer\n",
        "\n",
        "def linear(in_features, out_features, init_scale=1.0):\n",
        "    fc = nn.Linear(in_features, out_features)\n",
        "    with torch.no_grad():\n",
        "        fc.weight.data *= init_scale\n",
        "    return fc\n",
        "\n",
        "class DownsampleBlock(nn.Module):\n",
        "    # Block that doubles down on resolution. Use convolution block or average pooling block.\n",
        "    def __init__(self, channels, with_conv=True):\n",
        "        super().__init__()\n",
        "        if with_conv:\n",
        "            self.op = nn.Conv2d(channels, channels, kernel_size=3, stride=2, padding=1)\n",
        "        else:\n",
        "            self.op = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.op(x)\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    # Block that doubles the resolution. Use interpolating block.\n",
        "    def __init__(self, channels, with_conv=True):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if with_conv:\n",
        "            self.conv = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor=2.0, mode='nearest')\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Resnet block used in DDPM. Use group normalization. Timestep t is also received as input.\n",
        "\n",
        "    Parameters:\n",
        "        in_channels (int) : Number of input channels\n",
        "        out_channels (int) : Number of Output channels\n",
        "        temb_channels (int) : Number of time embedding channels\n",
        "        dropout (float) : Dropout rate\n",
        "        conv_shortcut (bool) : Whether to add convolution shortcut\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels=None,\n",
        "                 temb_channels=512, dropout=0.0, conv_shortcut=False):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels if out_channels is not None else in_channels\n",
        "        self.temb_channels = temb_channels\n",
        "        self.dropout = dropout\n",
        "        self.conv_shortcut = conv_shortcut\n",
        "\n",
        "        self.norm1 = GroupNorm32(self.in_channels)\n",
        "        self.conv1 = conv2d(self.in_channels, self.out_channels,\n",
        "                            kernel_size=3, stride=1, padding=1, init_scale=1.0)\n",
        "        self.temb_proj = linear(self.temb_channels, self.out_channels, init_scale=1.0)\n",
        "        self.norm2 = GroupNorm32(self.out_channels)\n",
        "        self.conv2 = conv2d(self.out_channels, self.out_channels,\n",
        "                            kernel_size=3, stride=1, padding=1, init_scale=0.0)\n",
        "\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.conv_shortcut:\n",
        "                self.conv_shortcut = nn.Conv2d(self.in_channels, self.out_channels,\n",
        "                                               kernel_size=3, stride=1, padding=1)\n",
        "            else:\n",
        "                self.conv_shortcut = nin(self.in_channels, self.out_channels)\n",
        "        else:\n",
        "            self.conv_shortcut = None\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            x (torch.Tensor) : Input data\n",
        "            temb (torch.Tensor) : Embedding vector of timestep t\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor : Output data\n",
        "\n",
        "        Example:\n",
        "            >>> x = torch.randn(128,3,224,224)\n",
        "            >>> temb = torch.randn(128,512)\n",
        "            >>> block = ResnetBlock(in_channels=3,out_channels=32,temb_channels=512)\n",
        "            >>> out = block(x,temb)\n",
        "            >>> print(out.shape) # torch.Size([128,32,224,224])\n",
        "        \"\"\"\n",
        "        h = self.norm1(x)\n",
        "        h = swish(h)\n",
        "        h = self.conv1(h)\n",
        "        h_temb = swish(temb)\n",
        "        h_temb = self.temb_proj(h_temb)  # (B, out_channels)\n",
        "        h_temb = h_temb[:, :, None, None]  # (B, out_channels, 1, 1)\n",
        "        h = h + h_temb\n",
        "        h = self.norm2(h)\n",
        "        h = swish(h)\n",
        "        h = F.dropout(h, p=self.dropout, training=self.training)\n",
        "        h = self.conv2(h)\n",
        "        if self.conv_shortcut is not None:\n",
        "            x = self.conv_shortcut(x)\n",
        "        return x + h\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention block used in DDPM.\n",
        "\n",
        "    Parameters:\n",
        "        channels (int) : Number of input, output channels\n",
        "    \"\"\"\n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.norm = GroupNorm32(channels)\n",
        "        self.q = nin(channels, channels)\n",
        "        self.k = nin(channels, channels)\n",
        "        self.v = nin(channels, channels)\n",
        "        self.proj_out = nin(channels, channels, init_scale=0.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            x (torch.Tensor) : Input data\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor : Output data\n",
        "\n",
        "        Example:\n",
        "            >>> x = torch.randn(128,32,224,224)\n",
        "            >>> block = AttnBlock(32)\n",
        "            >>> out = block(x)\n",
        "            >>> print(out.shape) # torch.Size([128,32,224,224])\n",
        "        \"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "        h = self.norm(x)\n",
        "        q = self.q(h).permute(0, 2, 3, 1)  # (B, H, W, C)\n",
        "        k = self.k(h).permute(0, 2, 3, 1)\n",
        "        v = self.v(h).permute(0, 2, 3, 1)\n",
        "        w = torch.einsum('bhwc,bHWc->bhwHW', q, k) * (C ** -0.5)\n",
        "        w = w.reshape(B, H, W, H * W)\n",
        "        w = F.softmax(w, dim=-1)\n",
        "        w = w.reshape(B, H, W, H, W)\n",
        "        h_ = torch.einsum('bhwHW,bHWc->bhwc', w, v)\n",
        "        h_ = self.proj_out(h_.permute(0, 3, 1, 2))\n",
        "        return x + h_\n",
        "\n",
        "class DDPMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Final DDPMModel. Modified version of UNet.\n",
        "    Attention block is applied to where it is set to applied.\n",
        "    Given noised data x_t and timestep t, the model estimates the value of noise at timestep t.\n",
        "\n",
        "    Parameters:\n",
        "        in_channels (int) : Number of input channels\n",
        "        out_channels (int) : Number of output channels\n",
        "        ch (int) : Number of default channel\n",
        "        ch_mult (tuple) : Coefficient multiblied by channels\n",
        "        num_res_blocks (int) : Number of resnet blocks\n",
        "        attn_resolutions (set) : Set of resolutions at which attention block applies\n",
        "        dropout (float) : Dropout rate\n",
        "        resamp_with_conv (bool) : Whether to use convolution whil down(up)sampling\n",
        "        init_resolution (int) : Resolution of input images\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        out_channels=3,\n",
        "        ch=64,\n",
        "        ch_mult=(1, 2, 4),\n",
        "        num_res_blocks=2,\n",
        "        attn_resolutions={8},\n",
        "        dropout=0.0,\n",
        "        resamp_with_conv=True,\n",
        "        init_resolution=32\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.ch = ch\n",
        "        self.ch_mult = ch_mult\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.attn_resolutions = attn_resolutions\n",
        "        self.dropout = dropout\n",
        "        self.num_levels = len(ch_mult)\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.resamp_with_conv = resamp_with_conv\n",
        "        self.init_resolution = init_resolution\n",
        "\n",
        "        # Dimension of time embedding vector\n",
        "        self.temb_ch = ch * 4\n",
        "\n",
        "        # Timestep embedding layers\n",
        "        self.temb_dense0 = linear(ch, self.temb_ch)\n",
        "        self.temb_dense1 = linear(self.temb_ch, self.temb_ch)\n",
        "\n",
        "        # Input conv\n",
        "        self.conv_in = conv2d(in_channels, ch, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Downsample blocks\n",
        "        # Each downsampling block(modulelist) is stored in down_blocks.\n",
        "        # Each block is composed of resnetblocks and attention block(if needed).\n",
        "        self.down_blocks = nn.ModuleList()\n",
        "        curr_ch = ch\n",
        "        curr_res = init_resolution\n",
        "        for level in range(self.num_levels):\n",
        "            level_blocks = nn.ModuleList()\n",
        "            out_ch = ch * ch_mult[level]\n",
        "            for i in range(num_res_blocks):\n",
        "                level_blocks.append(ResnetBlock(curr_ch, out_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                if curr_res in attn_resolutions:\n",
        "                    level_blocks.append(AttnBlock(out_ch))\n",
        "                curr_ch = out_ch\n",
        "            self.down_blocks.append(level_blocks)\n",
        "            if level != self.num_levels - 1:\n",
        "                self.down_blocks.append(DownsampleBlock(curr_ch, with_conv=resamp_with_conv))\n",
        "                curr_res //= 2\n",
        "\n",
        "        # Middle blocks\n",
        "        self.mid_block = nn.ModuleList([\n",
        "            ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout),\n",
        "            AttnBlock(curr_ch),\n",
        "            ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout)\n",
        "        ])\n",
        "\n",
        "        # Upsample blocks\n",
        "        # Symmetric structure with downsample blocks\n",
        "        self.up_blocks = nn.ModuleList()\n",
        "        for level in reversed(range(self.num_levels)):\n",
        "            level_blocks = nn.ModuleList()\n",
        "            out_ch = ch * ch_mult[level]\n",
        "            level_blocks.append(ResnetBlock(curr_ch + out_ch, out_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "            if (init_resolution // (2 ** level)) in attn_resolutions:\n",
        "                level_blocks.append(AttnBlock(out_ch))\n",
        "            curr_ch = out_ch\n",
        "            for i in range(num_res_blocks):\n",
        "                level_blocks.append(ResnetBlock(curr_ch, curr_ch, temb_channels=self.temb_ch, dropout=dropout))\n",
        "                if (init_resolution // (2 ** level)) in attn_resolutions:\n",
        "                    level_blocks.append(AttnBlock(curr_ch))\n",
        "            if level != 0:\n",
        "                level_blocks.append(UpsampleBlock(curr_ch, with_conv=resamp_with_conv))\n",
        "            self.up_blocks.append(level_blocks)\n",
        "\n",
        "        # output conv\n",
        "        self.norm_out = GroupNorm32(curr_ch)\n",
        "        self.conv_out = conv2d(curr_ch, out_channels, kernel_size=3, stride=1, padding=1, init_scale=0.0)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            x (torch.Tensor) : Input data\n",
        "            t (torch.Tensor) : Timesteps of batch data\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor : Output data\n",
        "\n",
        "        Example:\n",
        "            x = torch.randn(128,3,224,224)\n",
        "            t = torch.randint(0,1000,(128,))\n",
        "            model = DDPMModel(ch=128,attn_resolution={56})\n",
        "            out = model(x,t)\n",
        "            print(out.shape) # torch.Size([128,3,224,224])\n",
        "        \"\"\"\n",
        "        # 1) Timestep embedding\n",
        "        temb = get_timestep_embedding(t, self.ch)\n",
        "        temb = self.temb_dense0(temb)\n",
        "        temb = swish(temb)\n",
        "        temb = self.temb_dense1(temb)\n",
        "\n",
        "        # 2) Downsampling\n",
        "        skips = []\n",
        "        h = self.conv_in(x)\n",
        "        down_iter = iter(self.down_blocks)\n",
        "        for level in range(self.num_levels):\n",
        "            blocks = next(down_iter)\n",
        "            for layer in blocks:\n",
        "                h = layer(h, temb) if isinstance(layer, ResnetBlock) else layer(h)\n",
        "            skips.append(h)\n",
        "            if level != self.num_levels - 1:\n",
        "                downsample = next(down_iter)\n",
        "                h = downsample(h)\n",
        "\n",
        "        # 3) Middle\n",
        "        for layer in self.mid_block:\n",
        "            h = layer(h, temb) if isinstance(layer, ResnetBlock) else layer(h)\n",
        "\n",
        "        # 4) Upsampling\n",
        "        for level in range(self.num_levels):\n",
        "            blocks = self.up_blocks[level]\n",
        "            skip = skips.pop()\n",
        "            h = torch.cat([h, skip], dim=1)\n",
        "            h = blocks[0](h, temb)\n",
        "            for layer in blocks[1:]:\n",
        "                if isinstance(layer, ResnetBlock):\n",
        "                    h = layer(h, temb)\n",
        "                else:\n",
        "                    h = layer(h)\n",
        "\n",
        "        # 5) Output\n",
        "        h = self.norm_out(h)\n",
        "        h = swish(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n"
      ],
      "metadata": {
        "id": "NQ47alF9zYrv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Train"
      ],
      "metadata": {
        "id": "y62nABWwzavu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_beta_alpha(beta_schedule='linear', beta_start=0.0001, beta_end=0.002, num_timesteps=1000):\n",
        "    \"\"\"\n",
        "    Generate values of beta, alpha and bar_alpha.\n",
        "\n",
        "    Parameters:\n",
        "        beta_schedule (str) : Method of generating beta. 'linear' or 'quad'\n",
        "        beta_start (float) : Value of beta_0\n",
        "        beta_end (float) : Value of beta_T\n",
        "        num_timesteps (int) : Number of whole timesteps\n",
        "\n",
        "    Returns:\n",
        "        betas (torch.Tensor) : Value of betas\n",
        "        alphas (torch.Tensor) : Value of alphas\n",
        "        alphas_cumprod (torch.Tensor) : Value of bar_alpha\n",
        "    \"\"\"\n",
        "    if beta_schedule == 'linear':\n",
        "        betas = np.linspace(beta_start, beta_end, num_timesteps, dtype=np.float32)\n",
        "    elif beta_schedule == 'quad':\n",
        "        betas = (np.linspace(beta_start**0.5, beta_end**0.5, num_timesteps, dtype=np.float32)) ** 2\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Unknown beta schedule: {beta_schedule}\")\n",
        "\n",
        "    betas = torch.tensor(betas)\n",
        "    alphas = 1.0 - betas\n",
        "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "    return betas, alphas, alphas_cumprod\n",
        "\n",
        "def q_sample(x0, t, noise, alphas_cumprod):\n",
        "    \"\"\"\n",
        "    Return noised image x_t given noiseless data x_0, timestep t and the value of noise.\n",
        "\n",
        "    Parameters:\n",
        "        x0 (torch.Tensor) : Noiseless image data\n",
        "        t (torch.Tensor) : Timesteps of batch data\n",
        "        noise (torch.Tensor) : Noise data\n",
        "        alphas_cumprod (torch.Tensor) : Value of bar_alpha\n",
        "\n",
        "    Returns:\n",
        "        x_t (torch.Tensor) : Noised image data\n",
        "    \"\"\"\n",
        "    alpha_bar = alphas_cumprod[t].to(x0.device)\n",
        "    # Adjust shape of alpha_bar tensor\n",
        "    while len(alpha_bar.shape) < len(x0.shape):\n",
        "        alpha_bar = alpha_bar.unsqueeze(-1)\n",
        "\n",
        "    sqrt_alpha_bar = torch.sqrt(alpha_bar)\n",
        "    sqrt_one_minus_alpha_bar = torch.sqrt(1.0 - alpha_bar)\n",
        "\n",
        "    x_t = sqrt_alpha_bar * x0 + sqrt_one_minus_alpha_bar * noise\n",
        "\n",
        "    return x_t\n",
        "\n",
        "def compute_mse_loss(model, x_t, t, eps):\n",
        "    \"\"\"\n",
        "    Compute mse loss between output of the model and actual noise data.\n",
        "\n",
        "    Parameters:\n",
        "        model (torch.nn.Module) : DDPM model\n",
        "        x_t (torch.Tensor) : Noised image data\n",
        "        t (torch.Tensor) : Timesteps of batch data\n",
        "        eps (torch.Tensor) : Noise data\n",
        "\n",
        "    Returns:\n",
        "        loss (torch.Tensor) : MSE loss\n",
        "    \"\"\"\n",
        "    pred_eps = model(x_t, t)\n",
        "    loss = F.mse_loss(pred_eps, eps)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "fgNvvkLHzezO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x * 2.0 - 1.0)\n",
        "])\n",
        "\n",
        "batch_size = 128\n",
        "epoch = 5\n",
        "lr = 2e-4\n",
        "beta_schedule = 'linear'\n",
        "beta_start = 0.0001\n",
        "beta_end = 0.02\n",
        "num_timesteps = 1000\n",
        "\n",
        "device = torch.device('cuda:0'if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "betas, alphas, alphas_cumprod = get_beta_alpha(beta_schedule, beta_start, beta_end, num_timesteps)\n",
        "betas, alphas, alphas_cumprod = betas.to(device), alphas.to(device), alphas_cumprod.to(device)\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data',train=True,download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True,num_workers=2,drop_last=True)\n",
        "\n",
        "model = DDPMModel().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "for i in range(epoch):\n",
        "    loss_list = []\n",
        "    for image,_ in train_loader:\n",
        "        image = image.to(device)\n",
        "        eps = torch.randn(image.shape).to(device)\n",
        "        t = torch.randint(0, num_timesteps, (image.size(0),), dtype=torch.long).to(device)\n",
        "        x_t = q_sample(image, t, eps, alphas_cumprod)\n",
        "        loss = compute_mse_loss(model, x_t, t, eps)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "\n",
        "    print(f'{i}th epoch loss : {np.mean(loss_list)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8KD1t8NzhDk",
        "outputId": "15810736-6acb-4012-8ae1-f2ffacbf594b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 47.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "0th epoch loss : 0.1253967556815881\n",
            "1th epoch loss : 0.03896999595543513\n",
            "2th epoch loss : 0.03690607429314882\n",
            "3th epoch loss : 0.035248602396593644\n",
            "4th epoch loss : 0.03453492544686947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Sampling"
      ],
      "metadata": {
        "id": "KVVx5lIdzixH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def DDIM_Sampling(x, skip, model):\n",
        "    \"\"\"\n",
        "    Sample images with DDIM algorithm. Use fixed sigma=0 and subset indices.\n",
        "\n",
        "    Parameters:\n",
        "        x (torch.Tensor) : Random latent variable.\n",
        "        skip (int) :Size of skip steps. (skip==5 => seq=[0,5,10,...])\n",
        "        model (torch.nn.Module) : DDPM model\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor : Sampled images x_0\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        _,_,alpha = get_beta_alpha()\n",
        "        seq = list(range(skip,1000,skip)) + [999]\n",
        "        seq = reversed(seq)\n",
        "        B = x.size(0)\n",
        "        xs = [x]\n",
        "        for i in seq:\n",
        "            t = (torch.ones(B,dtype=torch.long) * i).to(x.device)\n",
        "            xt = xs[-1]\n",
        "            et = model(xt, t)\n",
        "            at = alpha[i]\n",
        "            x0_t = (xt - et * (1 - at).sqrt()) / at.sqrt()\n",
        "            if i==skip:\n",
        "                return x0_t\n",
        "            if i==999:\n",
        "                ni = 1000-skip\n",
        "            else:\n",
        "                ni = i - skip\n",
        "            at_next = alpha[ni]\n",
        "            c = (1 - at_next).sqrt()\n",
        "            xt_next = at_next.sqrt() * x0_t + c * et\n",
        "            xs.append(xt_next)"
      ],
      "metadata": {
        "id": "M3-4pAF2zkIx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Visualize"
      ],
      "metadata": {
        "id": "SnS-uKuPHzrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_sample(x0, idx=0):\n",
        "    \"\"\"\n",
        "    Visualize tensor data which is in the range of -1 to 1.\n",
        "\n",
        "    Parameters:\n",
        "        x0 (torch.Tensor) : Tensor data\n",
        "        idx (int) : Index of tensor data ('idx'th image in the batch)\n",
        "    \"\"\"\n",
        "    img = x0[idx]\n",
        "    img = img.detach().cpu().numpy()\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "    img = (img + 1.0) / 2.0\n",
        "    img = np.clip(img, 0, 1)\n",
        "\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Sampled x₀\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "HyS06xetHy02"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(128,3,32,32).to(device)\n",
        "x0 = DDIM_Sampling(x, 10, model)\n",
        "visualize_sample(x0, idx=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "CuCpziKeH11D",
        "outputId": "aaa2bd9d-3976-4bdf-a9ee-d0b17a21cc39"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}